Namespace(adam_epsilon=1e-08, batch_size_per_gpu=2, build_salient_entities=False, cache_path='/export/home/cache', concat_mode='concat_left', counterfactual_removal=False, cuda='3', data_dir='/export/home/dataset/PromptSumm/', dataset='xsum', dataset_cache_dir='/export/home/hf_datasets_v1/', dataset_name='xsum', dataset_version='default', debug=False, eval_step=100000, exp_id='007', few_shot=10, gradient_accumulation_steps=8, guidance_mode='target', guidance_type='ents', highlights=False, if_spacy=False, ifckpt_onlymodel=1, length_penalty=1.0, lm_adapted_path='/export/home/prompting/lm_adapted_models/t5.1.1.lm100k.large/pytorch_model.bin', local_rank=0, log_step=1, lr=0.5, max_epoch=30, max_grad_norm=1.0, max_guidance_length=100, max_length=512, max_summary_length=64, model='T5MixPrompt', model_name='google/t5-v1_1-large', num_beams=4, num_seeds=3, num_workers=0, pretrain_all_weights=True, pretrain_t5_tagger=True, pretrain_with_ent_chain=True, pretraining_train_size=204045, pretraining_val_size=1000, prompt_number=1, repetition_penalty=2.5, save_model=False, save_model_path='', save_step=100000, seed=42, separator=',', stemmer=True, summary_key='summary', test_key='test', test_size_per_gpu=8, text_key='document', train_sample=True, train_t5_tagger=False, use_bert_tagger=False, use_lm_adapted=1, use_pretrain_ckpt=True, use_t5_tagger=False, valid_size_per_gpu=16, validation_key='validation', warmup_steps=0.01, weight_decay=1e-05, zero_shot=False)
device cuda:0
04/26/2022 10:26:17 - INFO - __main__ -   gen token = summerizationcnndm , gen token id = 32100

pre-train tagger
Pre-training entity tagger...
pretrain_all_weights
Traceback (most recent call last):
  File "main.py", line 409, in <module>
    main(args)
  File "main.py", line 280, in main
    pretrain_model(dataset_args, args)
  File "/export/home/PromptSumm/Summarization/T5PromptNER/TrainTaggerforSum.py", line 591, in pretrain_model
    promptembedding = getpromptembedding(model, tokenizer, promptnumber, taskname)
  File "./T5PromptNER/NERDataset.py", line 269, in getpromptembedding
    promptinitembedding[startindex] = embeddingres
IndexError: index 1 is out of bounds for dimension 0 with size 1
Traceback (most recent call last):
  File "/export/home/anaconda/envs/a100/lib/python3.6/runpy.py", line 193, in _run_module_as_main
    "__main__", mod_spec)
  File "/export/home/anaconda/envs/a100/lib/python3.6/runpy.py", line 85, in _run_code
    exec(code, run_globals)
  File "/export/home/anaconda/envs/a100/lib/python3.6/site-packages/torch/distributed/launch.py", line 260, in <module>
    main()
  File "/export/home/anaconda/envs/a100/lib/python3.6/site-packages/torch/distributed/launch.py", line 256, in main
    cmd=cmd)
subprocess.CalledProcessError: Command '['/export/home/anaconda/envs/a100/bin/python', '-u', 'main.py', '--local_rank=0', '--pretrain_t5_tagger', '--valid_size_per_gpu', '16', '--batch_size_per_gpu', '2', '--cuda', '3', '--exp_id', '007', '--concat_mode', 'concat_left', '--pretrain_all_weights', '--prompt_number', '1', '--pretrain_with_ent_chain']' returned non-zero exit status 1.
Namespace(adam_epsilon=1e-08, batch_size_per_gpu=2, build_salient_entities=False, cache_path='/export/home/cache', concat_mode='concat_left', counterfactual_removal=False, cuda='3', data_dir='/export/home/dataset/PromptSumm/', dataset='xsum', dataset_cache_dir='/export/home/hf_datasets_v1/', dataset_name='xsum', dataset_version='default', debug=False, eval_step=100000, exp_id='007', few_shot=10, gradient_accumulation_steps=8, guidance_mode='target', guidance_type='ents', highlights=False, if_spacy=False, ifckpt_onlymodel=1, length_penalty=1.0, lm_adapted_path='/export/home/prompting/lm_adapted_models/t5.1.1.lm100k.large/pytorch_model.bin', local_rank=0, log_step=1, lr=0.5, max_epoch=30, max_grad_norm=1.0, max_guidance_length=100, max_length=512, max_summary_length=64, model='T5MixPrompt', model_name='google/t5-v1_1-large', num_beams=4, num_seeds=3, num_workers=0, pretrain_all_weights=True, pretrain_t5_tagger=True, pretrain_with_ent_chain=True, pretraining_train_size=204045, pretraining_val_size=1000, prompt_number=10, repetition_penalty=2.5, save_model=False, save_model_path='', save_step=100000, seed=42, separator=',', stemmer=True, summary_key='summary', test_key='test', test_size_per_gpu=8, text_key='document', train_sample=True, train_t5_tagger=False, use_bert_tagger=False, use_lm_adapted=1, use_pretrain_ckpt=True, use_t5_tagger=False, valid_size_per_gpu=16, validation_key='validation', warmup_steps=0.01, weight_decay=1e-05, zero_shot=False)
device cuda:0
04/26/2022 10:27:45 - INFO - __main__ -   gen token = summerizationcnndm , gen token id = 32100

pre-train tagger
Pre-training entity tagger...
pretrain_all_weights
prompt torch.Size([10, 1024])
04/26/2022 10:28:09 - INFO - T5PromptNER.TrainTaggerforSum -   The model has 783170560 trainable parameters
04/26/2022 10:28:12 - WARNING - datasets.builder -   Using custom data configuration default
04/26/2022 10:28:12 - WARNING - datasets.builder -   Reusing dataset xsum (/export/home/hf_datasets_v1/xsum/default/1.2.0/f9abaabb5e2b2a1e765c25417264722d31877b34ec34b437c53242f6e5c30d6d)
204045 1000
load the pre-training train data
204045
load the pre-training val data
1000
04/26/2022 10:28:23 - INFO - T5PromptNER.TrainTaggerforSum -   Begin pre-train...
04/26/2022 10:28:24 - INFO - root -   Training is not really distributed, single rank. Deactivating buckets
04/26/2022 10:28:24 - INFO - root -   ShardedDDP bucket size: 0.00M parameters, model size 746.89M parameters
04/26/2022 10:28:24 - INFO - T5PromptNER.TrainTaggerforSum -   0
04/26/2022 10:31:16 - INFO - T5PromptNER.TrainTaggerforSum -   step: 50,  lossent: 3.233123, losssum: 2.504476
04/26/2022 10:34:00 - INFO - T5PromptNER.TrainTaggerforSum -   step: 100,  lossent: 3.032538, losssum: 2.477146
04/26/2022 10:36:44 - INFO - T5PromptNER.TrainTaggerforSum -   step: 150,  lossent: 2.890856, losssum: 2.447295
04/26/2022 10:39:31 - INFO - T5PromptNER.TrainTaggerforSum -   step: 200,  lossent: 2.813257, losssum: 2.425259
04/26/2022 10:42:16 - INFO - T5PromptNER.TrainTaggerforSum -   step: 250,  lossent: 2.773270, losssum: 2.408441
04/26/2022 10:45:01 - INFO - T5PromptNER.TrainTaggerforSum -   step: 300,  lossent: 2.744839, losssum: 2.413189
04/26/2022 10:47:45 - INFO - T5PromptNER.TrainTaggerforSum -   step: 350,  lossent: 2.716088, losssum: 2.398607
04/26/2022 10:50:27 - INFO - T5PromptNER.TrainTaggerforSum -   step: 400,  lossent: 2.701673, losssum: 2.389376
04/26/2022 10:53:11 - INFO - T5PromptNER.TrainTaggerforSum -   step: 450,  lossent: 2.678789, losssum: 2.379301
04/26/2022 10:55:51 - INFO - T5PromptNER.TrainTaggerforSum -   step: 500,  lossent: 2.659665, losssum: 2.370141
04/26/2022 10:58:33 - INFO - T5PromptNER.TrainTaggerforSum -   step: 550,  lossent: 2.651926, losssum: 2.362706
04/26/2022 11:01:21 - INFO - T5PromptNER.TrainTaggerforSum -   step: 600,  lossent: 2.639889, losssum: 2.361130
04/26/2022 11:04:07 - INFO - T5PromptNER.TrainTaggerforSum -   step: 650,  lossent: 2.630042, losssum: 2.356425
04/26/2022 11:06:52 - INFO - T5PromptNER.TrainTaggerforSum -   step: 700,  lossent: 2.617338, losssum: 2.353841
04/26/2022 11:09:34 - INFO - T5PromptNER.TrainTaggerforSum -   step: 750,  lossent: 2.615150, losssum: 2.347807
04/26/2022 11:12:17 - INFO - T5PromptNER.TrainTaggerforSum -   step: 800,  lossent: 2.607618, losssum: 2.343278
04/26/2022 11:14:59 - INFO - T5PromptNER.TrainTaggerforSum -   step: 850,  lossent: 2.595228, losssum: 2.339591
04/26/2022 11:17:45 - INFO - T5PromptNER.TrainTaggerforSum -   step: 900,  lossent: 2.589297, losssum: 2.340580
04/26/2022 11:20:31 - INFO - T5PromptNER.TrainTaggerforSum -   step: 950,  lossent: 2.580322, losssum: 2.336008
04/26/2022 11:23:17 - INFO - T5PromptNER.TrainTaggerforSum -   step: 1000,  lossent: 2.574356, losssum: 2.328809
04/26/2022 11:26:03 - INFO - T5PromptNER.TrainTaggerforSum -   step: 1050,  lossent: 2.565848, losssum: 2.323338
04/26/2022 11:28:48 - INFO - T5PromptNER.TrainTaggerforSum -   step: 1100,  lossent: 2.559651, losssum: 2.320703
04/26/2022 11:31:33 - INFO - T5PromptNER.TrainTaggerforSum -   step: 1150,  lossent: 2.555773, losssum: 2.321431
04/26/2022 11:34:12 - INFO - T5PromptNER.TrainTaggerforSum -   step: 1200,  lossent: 2.556417, losssum: 2.320180
04/26/2022 11:36:54 - INFO - T5PromptNER.TrainTaggerforSum -   step: 1250,  lossent: 2.550578, losssum: 2.316775
04/26/2022 11:39:35 - INFO - T5PromptNER.TrainTaggerforSum -   step: 1300,  lossent: 2.548045, losssum: 2.315945
04/26/2022 11:42:17 - INFO - T5PromptNER.TrainTaggerforSum -   step: 1350,  lossent: 2.542678, losssum: 2.312975
04/26/2022 11:45:02 - INFO - T5PromptNER.TrainTaggerforSum -   step: 1400,  lossent: 2.541562, losssum: 2.307318
04/26/2022 11:47:50 - INFO - T5PromptNER.TrainTaggerforSum -   step: 1450,  lossent: 2.540249, losssum: 2.304887
04/26/2022 11:50:35 - INFO - T5PromptNER.TrainTaggerforSum -   step: 1500,  lossent: 2.536974, losssum: 2.301610
04/26/2022 11:53:16 - INFO - T5PromptNER.TrainTaggerforSum -   step: 1550,  lossent: 2.534794, losssum: 2.297808
04/26/2022 11:55:58 - INFO - T5PromptNER.TrainTaggerforSum -   step: 1600,  lossent: 2.532837, losssum: 2.296699
04/26/2022 11:58:22 - INFO - T5PromptNER.TrainTaggerforSum -   step: 1650,  lossent: 2.532525, losssum: 2.294535
04/26/2022 12:00:32 - INFO - T5PromptNER.TrainTaggerforSum -   step: 1700,  lossent: 2.529900, losssum: 2.292227
04/26/2022 12:02:39 - INFO - T5PromptNER.TrainTaggerforSum -   step: 1750,  lossent: 2.527141, losssum: 2.289538
04/26/2022 12:04:44 - INFO - T5PromptNER.TrainTaggerforSum -   step: 1800,  lossent: 2.524301, losssum: 2.288010
04/26/2022 12:06:50 - INFO - T5PromptNER.TrainTaggerforSum -   step: 1850,  lossent: 2.521685, losssum: 2.285846
04/26/2022 12:08:54 - INFO - T5PromptNER.TrainTaggerforSum -   step: 1900,  lossent: 2.516325, losssum: 2.284485
04/26/2022 12:11:06 - INFO - T5PromptNER.TrainTaggerforSum -   step: 1950,  lossent: 2.513086, losssum: 2.282315
04/26/2022 12:13:14 - INFO - T5PromptNER.TrainTaggerforSum -   step: 2000,  lossent: 2.510488, losssum: 2.279376
04/26/2022 12:15:22 - INFO - T5PromptNER.TrainTaggerforSum -   step: 2050,  lossent: 2.507039, losssum: 2.278607
04/26/2022 12:17:25 - INFO - T5PromptNER.TrainTaggerforSum -   step: 2100,  lossent: 2.505050, losssum: 2.277354
04/26/2022 12:19:38 - INFO - T5PromptNER.TrainTaggerforSum -   step: 2150,  lossent: 2.500650, losssum: 2.276307
04/26/2022 12:22:08 - INFO - T5PromptNER.TrainTaggerforSum -   step: 2200,  lossent: 2.498369, losssum: 2.274129
04/26/2022 12:24:52 - INFO - T5PromptNER.TrainTaggerforSum -   step: 2250,  lossent: 2.496449, losssum: 2.274116
04/26/2022 12:27:37 - INFO - T5PromptNER.TrainTaggerforSum -   step: 2300,  lossent: 2.494533, losssum: 2.271433
04/26/2022 12:30:18 - INFO - T5PromptNER.TrainTaggerforSum -   step: 2350,  lossent: 2.492264, losssum: 2.269525
04/26/2022 12:33:01 - INFO - T5PromptNER.TrainTaggerforSum -   step: 2400,  lossent: 2.491016, losssum: 2.267399
04/26/2022 12:35:42 - INFO - T5PromptNER.TrainTaggerforSum -   step: 2450,  lossent: 2.488622, losssum: 2.265820
04/26/2022 12:38:27 - INFO - T5PromptNER.TrainTaggerforSum -   step: 2500,  lossent: 2.487047, losssum: 2.263685
04/26/2022 12:41:06 - INFO - T5PromptNER.TrainTaggerforSum -   step: 2550,  lossent: 2.486608, losssum: 2.262842
04/26/2022 12:43:51 - INFO - T5PromptNER.TrainTaggerforSum -   step: 2600,  lossent: 2.484173, losssum: 2.260870
04/26/2022 12:46:31 - INFO - T5PromptNER.TrainTaggerforSum -   step: 2650,  lossent: 2.481692, losssum: 2.259372
04/26/2022 12:49:15 - INFO - T5PromptNER.TrainTaggerforSum -   step: 2700,  lossent: 2.481162, losssum: 2.258819
04/26/2022 12:51:59 - INFO - T5PromptNER.TrainTaggerforSum -   step: 2750,  lossent: 2.481237, losssum: 2.257276
04/26/2022 12:54:46 - INFO - T5PromptNER.TrainTaggerforSum -   step: 2800,  lossent: 2.479024, losssum: 2.257319
04/26/2022 12:57:32 - INFO - T5PromptNER.TrainTaggerforSum -   step: 2850,  lossent: 2.477647, losssum: 2.255998
04/26/2022 13:00:14 - INFO - T5PromptNER.TrainTaggerforSum -   step: 2900,  lossent: 2.476775, losssum: 2.255310
04/26/2022 13:02:53 - INFO - T5PromptNER.TrainTaggerforSum -   step: 2950,  lossent: 2.476031, losssum: 2.253876
04/26/2022 13:05:37 - INFO - T5PromptNER.TrainTaggerforSum -   step: 3000,  lossent: 2.475729, losssum: 2.253331
04/26/2022 13:08:22 - INFO - T5PromptNER.TrainTaggerforSum -   step: 3050,  lossent: 2.474196, losssum: 2.252504
04/26/2022 13:11:05 - INFO - T5PromptNER.TrainTaggerforSum -   step: 3100,  lossent: 2.472655, losssum: 2.251024
04/26/2022 13:13:51 - INFO - T5PromptNER.TrainTaggerforSum -   step: 3150,  lossent: 2.471562, losssum: 2.250563
04/26/2022 13:16:35 - INFO - T5PromptNER.TrainTaggerforSum -   step: 3200,  lossent: 2.470927, losssum: 2.249566
04/26/2022 13:19:19 - INFO - T5PromptNER.TrainTaggerforSum -   step: 3250,  lossent: 2.469234, losssum: 2.249164
04/26/2022 13:22:07 - INFO - T5PromptNER.TrainTaggerforSum -   step: 3300,  lossent: 2.469252, losssum: 2.247574
04/26/2022 13:24:51 - INFO - T5PromptNER.TrainTaggerforSum -   step: 3350,  lossent: 2.467076, losssum: 2.245871
04/26/2022 13:27:38 - INFO - T5PromptNER.TrainTaggerforSum -   step: 3400,  lossent: 2.465978, losssum: 2.245632
04/26/2022 13:30:19 - INFO - T5PromptNER.TrainTaggerforSum -   step: 3450,  lossent: 2.465395, losssum: 2.245504
04/26/2022 13:33:00 - INFO - T5PromptNER.TrainTaggerforSum -   step: 3500,  lossent: 2.464013, losssum: 2.244478
04/26/2022 13:35:44 - INFO - T5PromptNER.TrainTaggerforSum -   step: 3550,  lossent: 2.463714, losssum: 2.244134
04/26/2022 13:38:28 - INFO - T5PromptNER.TrainTaggerforSum -   step: 3600,  lossent: 2.462701, losssum: 2.243491
04/26/2022 13:41:11 - INFO - T5PromptNER.TrainTaggerforSum -   step: 3650,  lossent: 2.462405, losssum: 2.243536
04/26/2022 13:43:55 - INFO - T5PromptNER.TrainTaggerforSum -   step: 3700,  lossent: 2.461218, losssum: 2.242085
04/26/2022 13:46:35 - INFO - T5PromptNER.TrainTaggerforSum -   step: 3750,  lossent: 2.460757, losssum: 2.241492
04/26/2022 13:49:21 - INFO - T5PromptNER.TrainTaggerforSum -   step: 3800,  lossent: 2.460452, losssum: 2.240731
04/26/2022 13:52:02 - INFO - T5PromptNER.TrainTaggerforSum -   step: 3850,  lossent: 2.458825, losssum: 2.239575
04/26/2022 13:54:43 - INFO - T5PromptNER.TrainTaggerforSum -   step: 3900,  lossent: 2.456573, losssum: 2.238016
04/26/2022 13:57:26 - INFO - T5PromptNER.TrainTaggerforSum -   step: 3950,  lossent: 2.455856, losssum: 2.236892
04/26/2022 14:00:07 - INFO - T5PromptNER.TrainTaggerforSum -   step: 4000,  lossent: 2.454281, losssum: 2.236810
04/26/2022 14:00:07 - INFO - T5PromptNER.TrainTaggerforSum -   63
0it [00:00, ?it/s]0it [00:08, ?it/s]
Traceback (most recent call last):
  File "main.py", line 409, in <module>
    main(args)
  File "main.py", line 280, in main
    pretrain_model(dataset_args, args)
  File "/export/home/PromptSumm/Summarization/T5PromptNER/TrainTaggerforSum.py", line 753, in pretrain_model
    dooneevalforpretrain(model, valid_dataloader, args, scaler, result_dict, i, output_dir)
  File "/export/home/PromptSumm/Summarization/T5PromptNER/TrainTaggerforSum.py", line 460, in dooneevalforpretrain
    sensum, targetsum, predssum, sen, target, preds = model._generative_step(inputs_all)
  File "./T5PromptNER/NERModel.py", line 277, in _generative_step
    all_attention_mask = torch.cat([batch['entity_mask'], mask_prompt, attention_mask], 1)
NameError: name 'attention_mask' is not defined
Traceback (most recent call last):
  File "/export/home/anaconda/envs/a100/lib/python3.6/runpy.py", line 193, in _run_module_as_main
    "__main__", mod_spec)
  File "/export/home/anaconda/envs/a100/lib/python3.6/runpy.py", line 85, in _run_code
    exec(code, run_globals)
  File "/export/home/anaconda/envs/a100/lib/python3.6/site-packages/torch/distributed/launch.py", line 260, in <module>
    main()
  File "/export/home/anaconda/envs/a100/lib/python3.6/site-packages/torch/distributed/launch.py", line 256, in main
    cmd=cmd)
subprocess.CalledProcessError: Command '['/export/home/anaconda/envs/a100/bin/python', '-u', 'main.py', '--local_rank=0', '--pretrain_t5_tagger', '--valid_size_per_gpu', '16', '--batch_size_per_gpu', '2', '--cuda', '3', '--exp_id', '007', '--concat_mode', 'concat_left', '--pretrain_all_weights', '--prompt_number', '10', '--pretrain_with_ent_chain']' returned non-zero exit status 1.
Namespace(adam_epsilon=1e-08, batch_size_per_gpu=2, build_salient_entities=False, cache_path='/export/home/cache', concat_mode='concat_left', counterfactual_removal=False, cuda='3', data_dir='/export/home/dataset/PromptSumm/', dataset='xsum', dataset_cache_dir='/export/home/hf_datasets_v1/', dataset_name='xsum', dataset_version='default', debug=False, eval_step=100000, exp_id='007', few_shot=10, gradient_accumulation_steps=8, guidance_mode='target', guidance_type='ents', highlights=False, if_spacy=False, ifckpt_onlymodel=1, length_penalty=1.0, lm_adapted_path='/export/home/prompting/lm_adapted_models/t5.1.1.lm100k.large/pytorch_model.bin', local_rank=0, log_step=1, lr=0.5, max_epoch=30, max_grad_norm=1.0, max_guidance_length=100, max_length=512, max_summary_length=64, model='T5MixPrompt', model_name='google/t5-v1_1-large', num_beams=4, num_seeds=3, num_workers=0, pretrain_all_weights=True, pretrain_t5_tagger=True, pretrain_with_ent_chain=True, pretraining_train_size=204045, pretraining_val_size=1000, prompt_number=5, repetition_penalty=2.5, save_model=False, save_model_path='', save_step=100000, seed=42, separator=',', stemmer=True, summary_key='summary', test_key='test', test_size_per_gpu=8, text_key='document', train_sample=True, train_t5_tagger=False, use_bert_tagger=False, use_lm_adapted=1, use_pretrain_ckpt=True, use_t5_tagger=False, valid_size_per_gpu=16, validation_key='validation', warmup_steps=0.01, weight_decay=1e-05, zero_shot=False)
device cuda:0
04/27/2022 01:07:33 - INFO - __main__ -   gen token = summerizationcnndm , gen token id = 32100

pre-train tagger
Pre-training entity tagger...
pretrain_all_weights
prompt torch.Size([5, 1024])
04/27/2022 01:07:54 - INFO - T5PromptNER.TrainTaggerforSum -   The model has 783160320 trainable parameters
04/27/2022 01:07:58 - WARNING - datasets.builder -   Using custom data configuration default
04/27/2022 01:07:58 - WARNING - datasets.builder -   Reusing dataset xsum (/export/home/hf_datasets_v1/xsum/default/1.2.0/f9abaabb5e2b2a1e765c25417264722d31877b34ec34b437c53242f6e5c30d6d)
204045 1000
load the pre-training train data
204045
load the pre-training val data
1000
04/27/2022 01:08:07 - INFO - T5PromptNER.TrainTaggerforSum -   Begin pre-train...
04/27/2022 01:08:07 - INFO - root -   Training is not really distributed, single rank. Deactivating buckets
04/27/2022 01:08:07 - INFO - root -   ShardedDDP bucket size: 0.00M parameters, model size 746.88M parameters
04/27/2022 01:08:07 - INFO - T5PromptNER.TrainTaggerforSum -   0
04/27/2022 01:10:54 - INFO - T5PromptNER.TrainTaggerforSum -   step: 50,  lossent: 3.243502, losssum: 2.494792
04/27/2022 01:13:32 - INFO - T5PromptNER.TrainTaggerforSum -   step: 100,  lossent: 3.049653, losssum: 2.471435
Namespace(adam_epsilon=1e-08, batch_size_per_gpu=3, build_salient_entities=False, cache_path='/export/home/cache', concat_mode='concat_left', counterfactual_removal=False, cuda='3', data_dir='/export/home/dataset/PromptSumm/', dataset='xsum', dataset_cache_dir='/export/home/hf_datasets_v1/', dataset_name='xsum', dataset_version='default', debug=False, eval_step=100000, exp_id='007', few_shot=10, gradient_accumulation_steps=1, guidance_mode='target', guidance_type='ents', highlights=False, if_spacy=False, ifckpt_onlymodel=1, length_penalty=1.0, lm_adapted_path='/export/home/prompting/lm_adapted_models/t5.1.1.lm100k.large/pytorch_model.bin', local_rank=0, log_step=1, lr=0.5, max_epoch=30, max_grad_norm=1.0, max_guidance_length=100, max_length=512, max_summary_length=64, model='T5MixPrompt', model_name='google/t5-v1_1-large', num_beams=4, num_seeds=3, num_workers=0, pretrain_all_weights=True, pretrain_t5_tagger=True, pretrain_with_ent_chain=True, pretraining_train_size=204045, pretraining_val_size=1000, prompt_number=5, repetition_penalty=2.5, save_model=False, save_model_path='', save_step=100000, seed=42, separator=',', stemmer=True, summary_key='summary', test_key='test', test_size_per_gpu=8, text_key='document', train_sample=True, train_t5_tagger=False, use_bert_tagger=False, use_lm_adapted=1, use_pretrain_ckpt=True, use_t5_tagger=False, valid_size_per_gpu=16, validation_key='validation', warmup_steps=0.01, weight_decay=1e-05, zero_shot=False)
device cuda:0
04/27/2022 01:14:05 - INFO - __main__ -   gen token = summerizationcnndm , gen token id = 32100

pre-train tagger
Pre-training entity tagger...
pretrain_all_weights
prompt torch.Size([5, 1024])
04/27/2022 01:14:26 - INFO - T5PromptNER.TrainTaggerforSum -   The model has 783160320 trainable parameters
04/27/2022 01:14:28 - WARNING - datasets.builder -   Using custom data configuration default
04/27/2022 01:14:28 - WARNING - datasets.builder -   Reusing dataset xsum (/export/home/hf_datasets_v1/xsum/default/1.2.0/f9abaabb5e2b2a1e765c25417264722d31877b34ec34b437c53242f6e5c30d6d)
204045 1000
load the pre-training train data
204045
load the pre-training val data
1000
04/27/2022 01:14:38 - INFO - T5PromptNER.TrainTaggerforSum -   Begin pre-train...
04/27/2022 01:14:38 - INFO - root -   Training is not really distributed, single rank. Deactivating buckets
04/27/2022 01:14:38 - INFO - root -   ShardedDDP bucket size: 0.00M parameters, model size 746.88M parameters
04/27/2022 01:14:38 - INFO - T5PromptNER.TrainTaggerforSum -   0
04/27/2022 01:16:11 - INFO - T5PromptNER.TrainTaggerforSum -   step: 50,  lossent: 3.243315, losssum: 2.478891
04/27/2022 01:17:39 - INFO - T5PromptNER.TrainTaggerforSum -   step: 100,  lossent: 3.034740, losssum: 2.468206
04/27/2022 01:19:07 - INFO - T5PromptNER.TrainTaggerforSum -   step: 150,  lossent: 2.930526, losssum: 2.451910
04/27/2022 01:20:34 - INFO - T5PromptNER.TrainTaggerforSum -   step: 200,  lossent: 2.840713, losssum: 2.431232
